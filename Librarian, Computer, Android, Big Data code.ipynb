{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ae8528",
   "metadata": {},
   "source": [
    "# The Librarian, The Computer, The Android, and Big Data\n",
    "\n",
    "by Nichole Nomura and Quinn Dombrowski\n",
    "\n",
    "This Jupyter notebook contains the code used to search the *Star Trek* book corpus as described in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0c88b3",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572a3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for navigating directories\n",
    "import os\n",
    "#Used for identifying words\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "#Used for identifying sentences\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#Used for regular expressions\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d84ede8",
   "metadata": {},
   "source": [
    "## Navigate to directory with the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25143aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define directory with the corpus\n",
    "directory = '/Users/qad/Documents/dhtrek'\n",
    "#Change to the directory\n",
    "os.chdir(directory)\n",
    "#List all files in the directory\n",
    "files = os.listdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571276c",
   "metadata": {},
   "source": [
    "## Counting question words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda71089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create output file\n",
    "with open('/Users/qad/Documents/dhtrek-computer-questionwords.tsv', 'w') as out:\n",
    "    #Write header row\n",
    "    out.write('file' + '\\t' + 'word' + '\\t' + 'sentence' + '\\n')\n",
    "    #Iterate through files\n",
    "    for file in files:\n",
    "        #Only use text. files\n",
    "        if file.endswith('.txt'):\n",
    "            #Open the input file\n",
    "            with open(file, 'r') as inputfile:\n",
    "                #Print the file name\n",
    "                print(file)\n",
    "                #Read the input file\n",
    "                text = inputfile.read()\n",
    "                #Split text into sentences\n",
    "                sentences = tokenize.sent_tokenize(text)\n",
    "                #For each sentence...\n",
    "                for sentence in sentences:\n",
    "                    #If it contains 'Computer'\n",
    "                    if 'Computer' in sentence:\n",
    "                        #Lower-case the sentence\n",
    "                        sentence = sentence.lower()\n",
    "                        #Remove line breaks\n",
    "                        sentence = sentence.replace('\\n', ' ')\n",
    "                        #Split the sentence into words\n",
    "                        tokens = word_tokenize(sentence)\n",
    "                        #If the sentence includes 'why'...\n",
    "                        if 'why' in tokens:\n",
    "                            #Write out the sentence and the question word\n",
    "                            out.write (file + '\\t' + 'why' + '\\t' + sentence + '\\n')\n",
    "                        #If the sentence includes 'who'...\n",
    "                        if 'who' in tokens:\n",
    "                            #Write out the sentence and the question word\n",
    "                            out.write(file + '\\t' + 'who' + '\\t' + sentence + '\\n')\n",
    "                        #If the sentence includes 'what'...\n",
    "                        if 'what' in tokens:\n",
    "                            #Write out the sentence and the question word\n",
    "                            out.write(file + '\\t' + 'what' + '\\t' + sentence + '\\n')\n",
    "                        #If the sentence includes 'when'...\n",
    "                        if 'when' in tokens:\n",
    "                            #Write out the sentence and the question word\n",
    "                            out.write(file + '\\t' + 'when' + '\\t' + sentence + '\\n')\n",
    "                        #If the sentence includes 'where'...\n",
    "                        if 'where' in tokens:\n",
    "                            #Write out the sentence and the question word\n",
    "                            out.write(file + '\\t' + 'where' + '\\t' + sentence + '\\n')\n",
    "                        #If the sentence includes 'how'...\n",
    "                        if 'how' in tokens:\n",
    "                            #Write out the sentence and the question word\n",
    "                            out.write(file + '\\t' + 'how' + '\\t' + sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b306a",
   "metadata": {},
   "source": [
    "## Finding the commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates an empty list for sentences\n",
    "compsentences = []\n",
    "#Opens the output file\n",
    "with open('/Users/qad/Documents/dhtrek-computer-verbs.tsv', 'w') as out:\n",
    "    #Writes the header row\n",
    "    out.write('file' + '\\t' + 'word' + '\\t' + 'sentence' + '\\n')\n",
    "    #For each file\n",
    "    for file in files:\n",
    "        #If it's a text file\n",
    "        if file.endswith('.txt'):\n",
    "            #Open the text file\n",
    "            with open(file, 'r') as inputfile:\n",
    "                #Read the file\n",
    "                text = inputfile.read()\n",
    "                #Split the file into sentences\n",
    "                sentences = tokenize.sent_tokenize(text)\n",
    "                #For each sentence\n",
    "                for sentence in sentences:\n",
    "                    #If it includes 'Computer'\n",
    "                    if 'Computer' in sentence:\n",
    "                        #Replace new lines\n",
    "                        sentence = sentence.replace('\\n', ' ')\n",
    "                        #Regular expression to search for the next word after quote attribution\n",
    "                        compquote = re.search(r'Computer,”[A-Za-z ]*, “((\\w+))', sentence)\n",
    "                        #If that exists...\n",
    "                        if compquote is not None:\n",
    "                            #Write out the result\n",
    "                            out.write(file + '\\t' + compquote.group(1).lower() + '\\t' + sentence + '\\n')\n",
    "                        #Otherwise\n",
    "                        else:\n",
    "                            #Get the word that comes next\n",
    "                            compquote = re.search(r'Computer, ((\\w+))', sentence)\n",
    "                            #If that exists...\n",
    "                            if compquote is not None:\n",
    "                                #Write it out\n",
    "                                out.write(file + '\\t' + compquote.group(1).lower() + '\\t' + sentence + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e36ef4",
   "metadata": {},
   "source": [
    "## Contextual window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f96a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty list for sentences\n",
    "compsentences = []\n",
    "#Open the output file\n",
    "with open('/Users/qad/Documents/dhtrek-computer-verbs-context.tsv', 'w') as out:\n",
    "    #Write the header row\n",
    "    out.write('file' + '\\t' + 'word' + '\\t' + 'sentence' + '\\t' + 'context' + '\\n')\n",
    "    #For each file\n",
    "    for file in files:\n",
    "        #Only use text files\n",
    "        if file.endswith('.txt'):\n",
    "            #Open the text file\n",
    "            with open(file, 'r') as inputfile:\n",
    "                #Read the text file\n",
    "                text = inputfile.read()\n",
    "                #Split it into sentences\n",
    "                sentences = tokenize.sent_tokenize(text)\n",
    "                #For each sentence, getting the index of the sentence\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    #If 'Computer' is in the sentence\n",
    "                    if 'Computer' in sentence:\n",
    "                        #Get 3 sentences before and after\n",
    "                        contexts = sentences[max(i-3, 0):i+3]\n",
    "                        #Combine it all together\n",
    "                        output = ' '.join(contexts)\n",
    "                        #Strip newlines\n",
    "                        output = output.replace('\\n', ' ')\n",
    "                        # Get the word after 'Computer' if there's quote attribution\n",
    "                        compquote = re.search(r'Computer,”[A-Za-z ]*, “((\\w+))', sentence)\n",
    "                        #If that exists\n",
    "                        if compquote is not None:\n",
    "                            #Write the context out\n",
    "                            out.write(file + '\\t' + compquote.group(1).lower() + '\\t' + output + '\\n')\n",
    "                        #Otherwise...\n",
    "                        else:\n",
    "                            #Get the word after 'Computer'\n",
    "                            compquote = re.search(r'Computer, ((\\w+))', sentence)\n",
    "                            #If that exists\n",
    "                            if compquote is not None:\n",
    "                                #Write the context out\n",
    "                                out.write(file + '\\t' + compquote.group(1).lower() + '\\t' + output + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50719296",
   "metadata": {},
   "source": [
    "## Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99eae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list for sentences\n",
    "datasentences = []\n",
    "#Open the output file\n",
    "with open('/Users/qad/Documents/dhtrek-data-verbs-context.tsv', 'w') as out:\n",
    "    #Write the header row\n",
    "    out.write('file' + '\\t' + 'word' + '\\t' + 'sentence' + '\\t' + 'context' + '\\n')\n",
    "    #For each file\n",
    "    for file in files:\n",
    "        #If it is a text file\n",
    "        if file.endswith('.txt'):\n",
    "            #Open the file\n",
    "            with open(file, 'r') as inputfile:\n",
    "                #Read the file\n",
    "                text = inputfile.read()\n",
    "                #Split the file into sentences\n",
    "                sentences = tokenize.sent_tokenize(text)\n",
    "                #For each sentence, keeping track of its index\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    #If it includes 'Data'\n",
    "                    if 'Data' in sentence:\n",
    "                        #Get 3 sentences before/after\n",
    "                        contexts = sentences[max(i-3, 0):i+3]\n",
    "                        #Join the context sentences\n",
    "                        output = ' '.join(contexts)\n",
    "                        #Replace newlines\n",
    "                        output = output.replace('\\n', ' ')\n",
    "                        #Get the word after Data if there's quote attribution\n",
    "                        compquote = re.search(r'Data,”[A-Za-z ]*, “((\\w+))', sentence)\n",
    "                        #If that exists\n",
    "                        if compquote is not None:\n",
    "                            #Write out the result\n",
    "                            out.write(file + '\\t' + compquote.group(1).lower() + '\\t' + output + '\\n')\n",
    "                        #Otherwise\n",
    "                        else:\n",
    "                            #Get the word after Data\n",
    "                            compquote = re.search(r'Data, ((\\w+))', sentence)\n",
    "                            #If that exists\n",
    "                            if compquote is not None:\n",
    "                                #Write out the result\n",
    "                                out.write(file + '\\t' + compquote.group(1).lower() + '\\t' + output + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b8480",
   "metadata": {},
   "source": [
    "## The computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d0d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empty list for sentences\n",
    "compsentences = []\n",
    "#Open output file\n",
    "with open('/Users/qad/Documents/dhtrek-the-computer-context.tsv', 'w') as out:\n",
    "    #Write header row\n",
    "    out.write('file' + '\\t' + 'sentences' + '\\n')\n",
    "    #For each file\n",
    "    for file in files:\n",
    "        #If it's a text file\n",
    "        if file.endswith('.txt'):\n",
    "            #Open the file\n",
    "            with open(file, 'r') as inputfile:\n",
    "                #Read the file\n",
    "                text = inputfile.read()\n",
    "                #Split it into sentences\n",
    "                sentences = tokenize.sent_tokenize(text)\n",
    "                #For each sentence, keeping track of the index\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    #Lower-case the sentence\n",
    "                    sentence = sentence.lower()\n",
    "                    #If it includes 'the. computer'...\n",
    "                    if 'the computer' in sentence:\n",
    "                        #Get 3 sentences before and after\n",
    "                        contexts = sentences[max(i-3, 0):i+3]\n",
    "                        #Combine the context\n",
    "                        output = ' '.join(contexts)\n",
    "                        #Replace the newlines\n",
    "                        output = output.replace('\\n', ' ')\n",
    "                        #Write it to the output file\n",
    "                        out.write(file + '\\t' + output + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec20e2",
   "metadata": {},
   "source": [
    "## Computer, Librarian, Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the output file\n",
    "with open('/Users/qad/Documents/dhtrek-the-computer-librarian-archive.tsv', 'w') as out:\n",
    "    #Write out the header row\n",
    "    out.write('file' + '\\t' + 'thecomputer' + '\\t' + 'librarian' + '\\t' + 'archive' + '\\n')\n",
    "    #For each file...\n",
    "    for file in files:\n",
    "        #If it's a text file\n",
    "        if file.endswith('.txt'):\n",
    "            #Open the file\n",
    "            with open(file, 'r') as inputfile:\n",
    "                #Read the file\n",
    "                text = inputfile.read()\n",
    "                #Split it into sentences\n",
    "                sentences = tokenize.sent_tokenize(text)\n",
    "                #Create an empty list for computer sentences\n",
    "                compsentences = []\n",
    "                #Create an empty list for librarian sentences\n",
    "                librariansentences = []\n",
    "                #Create an empty list for archive sentences\n",
    "                archivesentences = []\n",
    "                #For each sentence, keeping track of the index\n",
    "                for i, sentence in enumerate(sentences):\n",
    "                    #Lower-case the sentence\n",
    "                    sentence = sentence.lower()\n",
    "                    #If it includes 'the computer'\n",
    "                    if 'the computer' in sentence:\n",
    "                        #Add it to the. computer list\n",
    "                        compsentences.append(sentence)\n",
    "                    #If it includes 'librarian'\n",
    "                    if 'librarian' in sentence:\n",
    "                        #Add it to the librarian list\n",
    "                        librariansentences.append(sentence)\n",
    "                    #If it includes 'the archive'\n",
    "                    if 'the archive' in sentence:\n",
    "                        #Add it to the archive list\n",
    "                        archivesentences.append(sentence)\n",
    "                #If there's 1 or more computer sentences\n",
    "                if len(compsentences) > 0:\n",
    "                    #Set comptuter status to 'y'\n",
    "                    compstatus = 'y'\n",
    "                #Otherwise\n",
    "                else:\n",
    "                    #Set computer status to 'n'\n",
    "                    compstatus = 'n'\n",
    "                #If there's 1 or more librarian sentences\n",
    "                if len(librariansentences) > 0:\n",
    "                    #Set librarian status to 'y'\n",
    "                    librarianstatus = 'y'\n",
    "                #Otherwise\n",
    "                else:\n",
    "                    #Set librarian status to 'n'\n",
    "                    librarianstatus = 'n'\n",
    "                #If there's 1 or more archive sentences\n",
    "                if len(archivesentences) > 0:\n",
    "                    #Set archive status to 'y'\n",
    "                    archivestatus = 'y'\n",
    "                #Otherwise\n",
    "                else:\n",
    "                    #Set archive status to 'n'\n",
    "                    archivestatus = 'n'\n",
    "                #Write out the result for that book\n",
    "                out.write(file + '\\t' + compstatus + '\\t' + librarianstatus + '\\t' + archivestatus + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
